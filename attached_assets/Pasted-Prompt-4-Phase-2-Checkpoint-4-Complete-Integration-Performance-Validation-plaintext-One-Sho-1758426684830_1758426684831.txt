Prompt 4: Phase 2, Checkpoint 4 â€” Complete Integration & Performance Validation
```plaintext
One-Shot Prompt: FireMode Phase 2, Checkpoint 4 - Integration & Performance (v2.2)

1. Primary Objective
Complete Phase 2 with full integration of all components, performance validation, and production readiness verification.

2. Implementation Steps

Step 4.1: Complete OpenTelemetry Integration
Update src/app/main.py:
```python
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

def setup_telemetry():
    trace.set_tracer_provider(TracerProvider())
    tracer_provider = trace.get_tracer_provider()
    
    # Configure OTLP exporter
    otlp_exporter = OTLPSpanExporter(
        endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "localhost:4317"),
        insecure=True
    )
    
    span_processor = BatchSpanProcessor(otlp_exporter)
    tracer_provider.add_span_processor(span_processor)
    
    # Instrument libraries
    FastAPIInstrumentor.instrument_app(app)
    SQLAlchemyInstrumentor().instrument(engine=engine)
    HTTPXClientInstrumentor().instrument()

# In main startup
@app.on_event("startup")
async def startup():
    setup_telemetry()
    # Start Go service supervisor
    await start_supervisor()
Step 4.2: Performance Monitoring
Create src/app/metrics/performance.py:
pythonimport time
from contextvars import ContextVar
from prometheus_client import Histogram, Counter, Gauge

# Metrics
request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint', 'status']
)

crdt_conflicts = Counter(
    'crdt_conflicts_total',
    'Total CRDT merge conflicts',
    ['session_id']
)

sync_latency = Histogram(
    'sync_latency_seconds',
    'Offline sync latency',
    ['client_type']
)

bundle_size = Histogram(
    'offline_bundle_bytes',
    'Size of offline bundles',
    ['session_id']
)

active_connections = Gauge(
    'active_connections',
    'Number of active connections'
)

# Context for request tracking
request_id_var: ContextVar[str] = ContextVar('request_id', default='')

async def track_performance(request, call_next):
    start = time.time()
    
    # Generate request ID
    request_id = str(uuid.uuid4())
    request_id_var.set(request_id)
    
    try:
        response = await call_next(request)
        duration = time.time() - start
        
        # Record metrics
        request_duration.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).observe(duration)
        
        # Check p95 latency requirement
        if request.url.path == "/v1/tests/sessions/results" and duration > 0.3:
            logger.warning(f"Performance violation: {duration}s > 300ms requirement")
        
        return response
    finally:
        active_connections.dec()
Step 4.3: Final Load Test for TDD Compliance
Create tests/load/final_validation.py:
pythonfrom locust import HttpUser, task, between, events
import numpy as np

class ProductionLoadTest(HttpUser):
    wait_time = between(0.5, 1.5)
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.latencies = []
    
    @task(10)
    def classify_fault(self):
        """Primary performance-critical endpoint"""
        start = time.time()
        response = self.client.post(
            "/v1/classify",
            json={
                "item_code": "AS1851-2012-FE-01",
                "observed_condition": "pressure_low"
            },
            headers={"Authorization": f"Bearer {self.token}"}
        )
        latency = time.time() - start
        self.latencies.append(latency)
    
    @task(5)
    def submit_results(self):
        """CRDT submission endpoint"""
        response = self.client.post(
            "/v1/tests/sessions/123/results",
            json={
                "changes": [{"op": "set", "path": "/test", "value": "data"}],
                "_sync_meta": {"vector_clock": {}}
            },
            headers={
                "Authorization": f"Bearer {self.token}",
                "Idempotency-Key": str(uuid.uuid4())
            }
        )
    
    @task(2)
    def get_offline_bundle(self):
        """Offline bundle generation"""
        response = self.client.get(
            "/v1/tests/sessions/123/offline_bundle",
            headers={"Authorization": f"Bearer {self.token}"}
        )
        
        # Verify bundle size < 50MB
        if response.status_code == 200:
            size = len(response.content)
            if size > 50 * 1024 * 1024:
                response.failure(f"Bundle too large: {size} bytes")

@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    """Validate p95 latency requirement"""
    all_latencies = []
    for user in environment.runner.user_instances:
        all_latencies.extend(user.latencies)
    
    if all_latencies:
        p95 = np.percentile(all_latencies, 95)
        print(f"P95 Latency: {p95:.3f}s")
        if p95 > 0.3:
            print("FAILED: P95 latency exceeds 300ms requirement")
        else:
            print("PASSED: P95 latency meets requirement")